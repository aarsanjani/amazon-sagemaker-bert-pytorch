{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune and host PyTorch BERT model on SageMaker\n",
    "\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK provides open source APIs and containers that make it easy to train and deploy models in SageMaker with several different machine learning and deep learning frameworks.\n",
    "\n",
    "For this example, we use an Amazon SageMaker Notebook Instance for running the code. For information on how to use Amazon SageMaker Notebook Instances, see the AWS documentation (https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, os, pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-bert'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data\n",
    "\n",
    "We use Corpus of Linguistic Acceptability (CoLA) (https://nyu-mll.github.io/CoLA/), a dataset of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. We download and unzip the data using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "    !curl -o ./cola_public_1.1.zip https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
    "if not os.path.exists('./cola_public/'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentence and label\n",
    "\n",
    "Let us take a quick look at our data. First we read in the training data. The only two columns we need are the sentence itself and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./cola_public/raw/in_domain_train.tsv',\n",
    "                 sep='\\t',header=None, usecols=[1,3], names=['label','sentence'])\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[20:25])\n",
    "print(labels[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the dataset for training and testing before uploading both to Amazon S3 for use later. The SageMaker Python SDK provides a helpful class for uploading to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)\n",
    "train.to_csv('./cola_public/train.csv', index=False)\n",
    "test.to_csv('./cola_public/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing data is uploaded to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data('./cola_public/train.csv', bucket=bucket,key_prefix=prefix)\n",
    "inputs_test = sagemaker_session.upload_data('./cola_public/test.csv', bucket=bucket,key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we use the PyTorch estimator class to train our model. When creating our estimator, we make sure to specify a few things:\n",
    "\n",
    "* entry_point: the name of our PyTorch script. It contains training script loads data from the input channels, configures training with hyperparameters, trains a model, saves a model, loads and runs model during inference.\n",
    "* source_dir: the location of our training scripts and requirements.txt file. \"requirements.txt\" lists packages you want to use with your script.\n",
    "\n",
    "\n",
    "We use PyTorch-Transformers library (https://pytorch.org/hub/huggingface_pytorch-transformers), which contains PyTorch implementations and pre-trained model weights for many NLP models, including BERT.\n",
    "\n",
    "Our training script should save model artifacts learned during training to a file path called `model_dir`, as stipulated by the SageMaker PyTorch image. Upon completion of training, model artifacts saved in `model_dir` will be uploaded to S3 by SageMaker and will become available in S3 for deployment.\n",
    "\n",
    "We save this script in a file named `train_deploy.py`, and put the file in a directory named `code`. The full training script can be viewed under `/code` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train_deploy.py',\n",
    "                    source_dir='code',\n",
    "                    role=role,\n",
    "                    framework_version='1.3.1',\n",
    "                    train_instance_count=2,  # this script only support distributed training for GPU instances.\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'num_labels':2,\n",
    "                        'backend': 'gloo'\n",
    "                    })\n",
    "\n",
    "estimator.fit({'training': inputs_train, 'testing':inputs_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we host it on an Amazon SageMaker Endpoint. To make the endpoint load the model and serve predictions, we implement a few methods in `train_deploy.py`.\n",
    "\n",
    "* `model_fn()`: function defined to load the saved model and return a model object that can be used for model serving. The SageMaker PyTorch model server loads our model by invoking model_fn.\n",
    "* `input_fn()`: deserializes and prepares the prediction input. In this example, our request body is first serialized to JSON and then sent to model serving endpoint. Therefore, in `input_fn()`, we first deserialize the JSON-formatted request body and return the input as a `torch.tensor`, as required for BERT.\n",
    "* `predict_fn()`: performs the prediction and returns the result.\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our PyTorch estimator object, passing in our desired number of instances and instance type:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             instance_type='ml.g4dn.xlarge',\n",
    "                             endpoint_name='g4dn-xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the predictor to use `application/json` for the content type when sending requests to our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_deserializer, json_serializer\n",
    "\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.accept = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict('Somebody just left - guess who.')\n",
    "print(np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use model that have been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reuse pretrained model, you can create PyTorchModel from existing model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel \n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='<S3 location>/model.tar.gz',\n",
    "                             role=role,\n",
    "                             framework_version='1.3.1',\n",
    "                             source_dir='code',\n",
    "                             entry_point='train_deploy.py')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.p2.8xlarge', initial_instance_count=20, endpoint_name='p2.8xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Elastic Inference (https://aws.amazon.com/machine-learning/elastic-inference/) solves this problem by enabling you to attach the right amount of GPU-powered inference acceleration to any Amazon SageMaker (https://aws.amazon.com/sagemaker/) or EC2 (http://aws.amazon.com/ec2) instance, or Amazon ECS (http://aws.amazon.com/ecs) task. PyTorch is supported by Elastic Inference since Mar 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Elastic Inference, we must convert our trained model to TorchScript. The location of the model artifacts is `estimator.model_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a folder to save model trained model. We download the model.tar.gz file to local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $estimator.model_data\n",
    "mkdir model\n",
    "aws s3 cp $1 model/ \n",
    "tar xvzf model/model.tar.gz --directory ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code convert our model into TorchScript format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model_torchScript = BertForSequenceClassification.from_pretrained('model/', torchscript=True)\n",
    "device = 'cpu'\n",
    "for_jit_trace_input_ids = [0] * 64\n",
    "for_jit_trace_attention_masks = [0] * 64\n",
    "for_jit_trace_input, for_jit_trace_masks = torch.tensor([for_jit_trace_input_ids]), torch.tensor([for_jit_trace_input_ids])\n",
    "\n",
    "# Creating the trace\n",
    "traced_model = torch.jit.trace(model_torchScript, [for_jit_trace_input.to(device), for_jit_trace_masks.to(device)])\n",
    "torch.jit.save(traced_model, 'traced_bert.pt')\n",
    "\n",
    "subprocess.call(['tar', '-czvf', 'traced_bert.tar.gz', 'traced_bert.pt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we upload TorchScript model to s3 and deploy using Elastic Inference. Loading the TorchScript model and using it for prediction require small changes in our model loading and prediction functions. We create a new script `deploy_EI.py` that is slightly different from `train_deploy.py` script. The accelerator_type=`ml.eia2.xlarge` parameter is how we attach the Elastic Inference accelerator to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "instance_type = 'm5.large'\n",
    "accelerator_type = 'eia2.xlarge'\n",
    "\n",
    "# TorchScript model\n",
    "tar_filename = 'traced_bert.tar.gz'\n",
    "\n",
    "# Returns S3 bucket URL\n",
    "print('Upload tarball to S3')\n",
    "model_data = sagemaker_session.upload_data(path=tar_filename)\n",
    "\n",
    "endpoint_name = 'bert-ei-traced-{}-{}'.format(instance_type, accelerator_type).replace('.', '').replace('_', '')\n",
    "\n",
    "pytorch = PyTorchModel(model_data=model_data,\n",
    "                       role=role,\n",
    "                       source_dir='code',\n",
    "                       framework_version='1.3.1',\n",
    "                       py_version='py3',\n",
    "                       entry_point='deploy_ei.py',\n",
    "                       sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Function will exit before endpoint is finished creating\n",
    "predictor = pytorch.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.' + instance_type,\n",
    "                           accelerator_type='ml.' + accelerator_type,\n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don't forget delete endpoints afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
